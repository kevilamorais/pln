{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevilamorais/pln/blob/main/toxic_discourse_dl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "VucHBsg4vZdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsyZYVgpvPC4"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install emoji\n",
        "\n",
        "!python -m spacy download pt_core_news_sm "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import emoji\n",
        "import os\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from datasets import load_dataset\n",
        "from unicodedata import normalize\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "G-Wr1mJhwLQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset: https://huggingface.co/datasets/told-br"
      ],
      "metadata": {
        "id": "8_wnR92uwcUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('told-br')\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "OklqyQjXvZuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_rows = 1000\n",
        "\n",
        "tweets_train = dataset['train']['text']\n",
        "tweets_validation = dataset['validation']['text']\n",
        "tweets_test = dataset['test']['text']\n",
        "\n",
        "labels_train = dataset['train']['label']\n",
        "labels_validation = dataset['validation']['label']\n",
        "labels_test = dataset['test']['label']\n",
        "\n",
        "if n_rows > 0:\n",
        "  tweets_train = tweets_train[:n_rows]\n",
        "  tweets_validation = tweets_validation[:n_rows]\n",
        "  tweets_test = tweets_test[:n_rows]\n",
        "\n",
        "  labels_train = labels_train[:n_rows]\n",
        "  labels_validation = labels_validation[:n_rows]\n",
        "  labels_test = labels_test[:n_rows]\n",
        "\n",
        "print(f'\\nTrain: {len(tweets_train)}')\n",
        "print(f'Validation: {len(tweets_validation)}')\n",
        "print(f'Test: {len(tweets_test)}')\n",
        "\n",
        "print(f'\\n\\nLabels Distribution Train: {Counter(labels_train)}')\n",
        "print(f'Labels Distribution Validation: {Counter(labels_validation)}')\n",
        "print(f'Labels Distribution Test: {Counter(labels_test)}')"
      ],
      "metadata": {
        "id": "wXLYtQBrxCfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Raw Tweet Train: {tweets_train[0]}')\n",
        "print(f'\\nRaw Tweet Validation: {tweets_validation[0]}')\n",
        "print(f'\\nRaw Tweet Test: {tweets_test[0]}')"
      ],
      "metadata": {
        "id": "FM078mm8xHUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessar_tweets(tweets):\n",
        "    nlp = spacy.load('pt_core_news_sm')\n",
        "    repetion_pattern = re.compile(r'(.)\\1\\1+')\n",
        "    new_tweets = []\n",
        "    with tqdm(total=len(tweets), colour='green', desc='Processando') as pbar:\n",
        "      for tweet in tweets:\n",
        "          tweet = emoji.demojize(tweet, language='pt')\n",
        "          tweet = tweet.replace('_', ' ')\n",
        "          tweet = normalize('NFKD', tweet).encode('ASCII', 'ignore').decode('ASCII')\n",
        "          tweet = repetion_pattern.sub(r'\\1', tweet)\n",
        "          tweet = re.sub(r'https?://\\w+', '', tweet)\n",
        "          tweet = re.sub(r'@\\w+', ' ', tweet)\n",
        "          tweet = re.sub(r'\\s\\s+', ' ', tweet)\n",
        "          doc = nlp(tweet)\n",
        "          tokens = [t.lemma_.lower() for t in doc if t.pos_ != 'PUNCT' and \\\n",
        "                    not t.is_stop and len(t.lemma_) > 1]\n",
        "          new_tweet = ' '.join(tokens)\n",
        "          new_tweets.append(new_tweet.strip())\n",
        "          pbar.update(1)\n",
        "    return new_tweets"
      ],
      "metadata": {
        "id": "EyyF8UH8xeVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_train = preprocessar_tweets(tweets_train)\n",
        "tweets_validation = preprocessar_tweets(tweets_validation)\n",
        "tweets_test = preprocessar_tweets(tweets_test)\n",
        "\n",
        "print(f'\\n\\nPreprocessed Tweet Train: {tweets_train[0]}')\n",
        "print(f'Preprocessed Tweet Validation: {tweets_validation[0]}')\n",
        "print(f'Preprocessed Tweet Test: {tweets_test[0]}')"
      ],
      "metadata": {
        "id": "aYQhOQlhyHbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(oov_token='<oov>')\n",
        "\n",
        "tokenizer.fit_on_texts(tweets_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(tweets_train)\n",
        "X_val = tokenizer.texts_to_sequences(tweets_validation)\n",
        "X_test = tokenizer.texts_to_sequences(tweets_test)\n",
        "\n",
        "print(f'\\n\\nSeq. Tweet Train: {X_train[0]}')\n",
        "print(f'Seq. Tweet Validation: {X_val[0]}')\n",
        "print(f'Seq. Tweet Test: {X_test[0]}')"
      ],
      "metadata": {
        "id": "3VxhazDgyNbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max([len(x) for x in X_train])\n",
        "\n",
        "print(max_len)\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=max_len, padding='post')\n",
        "X_val = pad_sequences(X_val, maxlen=max_len, padding='post')\n",
        "X_test = pad_sequences(X_test, maxlen=max_len, padding='post')\n",
        "\n",
        "print(f'\\n\\nSeq. Padded Tweet Train: {X_train[0]}')\n",
        "print(f'Seq. Padded Tweet Validation: {X_val[0]}')\n",
        "print(f'Seq. Padded Tweet Test: {X_test[0]}')"
      ],
      "metadata": {
        "id": "u4b9HIbt2X3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_simple_model(max_len, num_classes):\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(units=256, input_shape=(max_len,), activation='relu'))\n",
        "  model.add(layers.Dense(units=num_classes, activation='sigmoid'))\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def build_simple_model_embedding(max_len, vocab_size, embedding_dim, num_classes):\n",
        "   model = models.Sequential()\n",
        "   model.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, \\\n",
        "                              input_length=max_len))\n",
        "   model.add(layers.Flatten())\n",
        "   model.add(layers.Dense(units=64, activation='relu'))\n",
        "   model.add(layers.Dense(units=num_classes, activation='sigmoid'))\n",
        "   model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "   return model"
      ],
      "metadata": {
        "id": "G0JoVNB17Aak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "num_classes = 2\n",
        "\n",
        "print('\\nVocab size:', vocab_size)\n",
        "\n",
        "y_train = to_categorical(labels_train, num_classes=num_classes)\n",
        "y_val = to_categorical(labels_validation, num_classes=num_classes)\n",
        "\n",
        "print('\\nTrain Labels:', labels_train[0], '-', y_train[0])\n",
        "print('Validation Labels:', labels_validation[0], '-', y_val[0])"
      ],
      "metadata": {
        "id": "S2Lnx_Ur4O64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_option = 2\n",
        "\n",
        "if model_option == 1:\n",
        "  model = build_simple_model(max_len, num_classes)\n",
        "elif model_option == 2:\n",
        "  model = build_simple_model_embedding(max_len, vocab_size, embedding_dim=100, \\\n",
        "                                       num_classes=num_classes)\n",
        "elif model_option == 3:\n",
        "  pass\n",
        "elif model_option == 4:\n",
        "  pass\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "BZrEQyNtAlqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = '/content/model_checkpoint/'\n",
        "\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "model_checkpoint = ModelCheckpoint(filepath=checkpoint_dir, \\\n",
        "                                   save_weights_only=True, monitor='val_accuracy', \\\n",
        "                                   mode='max', save_best_only=True)\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=128, epochs=10, \\\n",
        "                    validation_data=(X_val, y_val), callbacks=[model_checkpoint])\n",
        "\n",
        "model.load_weights(checkpoint_dir)"
      ],
      "metadata": {
        "id": "Bb39Un--6Ibv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NtqJudcHClZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HmFfM8usCusI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "y_pred = np.argmax(y_pred, axis=1)"
      ],
      "metadata": {
        "id": "5rCKQZ7yEpRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report = classification_report(labels_test, y_pred, zero_division=0)\n",
        "\n",
        "print(report)"
      ],
      "metadata": {
        "id": "inWRij2TE9pB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ConfusionMatrixDisplay.from_predictions(labels_test, y_pred)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z-ccADWlF_XJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TAz9-OMaGMA7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}